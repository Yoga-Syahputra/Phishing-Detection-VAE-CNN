{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcnGfWi5-eDb"
      },
      "source": [
        "**Mount to Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_sZlk3u-ovY",
        "outputId": "e72cf787-bbe8-4710-b093-01150ef62929"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cZEDvNdnqpc"
      },
      "source": [
        "**Import Necessary Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yV_legwEBhn5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edQ2GKL5Monq"
      },
      "source": [
        "1. Load the CSV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ev2EIc5uM1_O"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Thesis - Undergraduate Ch./Dataset/Phishing/Raw/Crawled-Phishing-URLs-Batch1.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K71SvoIzM6Mu"
      },
      "source": [
        "2. Inspect the DataFrame to confirm column names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBMiv-ZDM5gb",
        "outputId": "75992397-f7f4-421b-c4c1-01e4e18936c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns available: ['conversation_id_str', 'created_at', 'favorite_count', 'full_text', 'id_str', 'image_url', 'in_reply_to_screen_name', 'lang', 'location', 'quote_count', 'reply_count', 'retweet_count', 'tweet_url', 'user_id_str', 'username']\n",
            "   conversation_id_str                      created_at  favorite_count  \\\n",
            "0  1875784676901842949  Sun Jan 05 06:01:32 +0000 2025               0   \n",
            "1  1877217545263968616  Thu Jan 09 04:55:14 +0000 2025               1   \n",
            "2  1877214318065766421  Thu Jan 09 04:42:25 +0000 2025               0   \n",
            "3  1877215200186708314  Thu Jan 09 04:45:55 +0000 2025               2   \n",
            "4  1877217204954911165  Thu Jan 09 04:53:53 +0000 2025               0   \n",
            "\n",
            "                                           full_text               id_str  \\\n",
            "0  #phishing ALERT https://sigwgb[.]net https://t...  1875784676901842949   \n",
            "1  #phishing ALERT https://authorizedqfsbond[.]co...  1877217545263968616   \n",
            "2  #phishing ALERT https://airdrop-tronnetwork[.]...  1877214318065766421   \n",
            "3  #phishing ALERT https://amlwalletreports[.]com...  1877215200186708314   \n",
            "4  #phishing ALERT https://robiox[.]top https://t...  1877217204954911165   \n",
            "\n",
            "                                         image_url  in_reply_to_screen_name  \\\n",
            "0  https://pbs.twimg.com/media/GgggN58XwAA5B39.png                      NaN   \n",
            "1  https://pbs.twimg.com/media/Gg03Z8lWYAAeUIv.png                      NaN   \n",
            "2  https://pbs.twimg.com/media/Gg00d9RW0AAVDjn.png                      NaN   \n",
            "3  https://pbs.twimg.com/media/Gg01RWIWUAA9Keb.png                      NaN   \n",
            "4  https://pbs.twimg.com/media/Gg03GKDWUAAU8yk.png                      NaN   \n",
            "\n",
            "  lang location  quote_count  reply_count  retweet_count  \\\n",
            "0   no       US            0            0              0   \n",
            "1   no       US            0            0              0   \n",
            "2   no       US            0            0              0   \n",
            "3   no       US            0            0              0   \n",
            "4   no       US            0            0              0   \n",
            "\n",
            "                                           tweet_url          user_id_str  \\\n",
            "0  https://x.com/CarlyGriggs13/status/18757846769...  1558446401440231424   \n",
            "1  https://x.com/CarlyGriggs13/status/18772175452...  1558446401440231424   \n",
            "2  https://x.com/CarlyGriggs13/status/18772143180...  1558446401440231424   \n",
            "3  https://x.com/CarlyGriggs13/status/18772152001...  1558446401440231424   \n",
            "4  https://x.com/CarlyGriggs13/status/18772172049...  1558446401440231424   \n",
            "\n",
            "        username  \n",
            "0  CarlyGriggs13  \n",
            "1  CarlyGriggs13  \n",
            "2  CarlyGriggs13  \n",
            "3  CarlyGriggs13  \n",
            "4  CarlyGriggs13  \n"
          ]
        }
      ],
      "source": [
        "print(\"Columns available:\", df.columns.tolist())\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca0oYexpOUT0"
      },
      "source": [
        "3. Select the column containing the tweet text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EPMD17RnmhK"
      },
      "outputs": [],
      "source": [
        "if 'full_text' in df.columns:\n",
        "    tweets = df['full_text']\n",
        "else:\n",
        "    raise ValueError(\"The expected column 'full text' was not found in your CSV.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W73BGIXxOckm"
      },
      "source": [
        "4. Define a function to extract the suspected phishing URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rm-HnEWOdF0"
      },
      "outputs": [],
      "source": [
        "def extract_phishing_url(tweet):\n",
        "    \"\"\"\n",
        "    Extract the phishing URL from a tweet's text.\n",
        "    It selects the first URL that does not belong to 't.co'\n",
        "    and cleans it by replacing '[.]' with '.'.\n",
        "    \"\"\"\n",
        "    if isinstance(tweet, str):\n",
        "        # Regex to extract URLs starting with http:// or https://\n",
        "        urls = re.findall(r'https?://[^\\s,]+', tweet)\n",
        "        for url in urls:\n",
        "            # Skip URLs from the shortener (t.co)\n",
        "            if \"t.co\" not in url:\n",
        "                # Clean the phishing obfuscation by replacing '[.]' with '.'\n",
        "                return url.replace('[.]', '.')\n",
        "        # Return an empty string if none of the URLs match our criterion\n",
        "        return ''\n",
        "    else:\n",
        "        return ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GTfjeCfO12m"
      },
      "source": [
        "5. Apply the function to each tweet to create a new column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMBstmeBO1-0",
        "outputId": "a379c298-e2db-4e00-c478-730f9bb1b406"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Phishing URLs:\n",
            "0                 https://sigwgb.net\n",
            "1      https://authorizedqfsbond.com\n",
            "2    https://airdrop-tronnetwork.com\n",
            "3       https://amlwalletreports.com\n",
            "4                 https://robiox.top\n",
            "5             https://thefarm.today/\n",
            "6                https://gekkoai.xyz\n",
            "7       https://stake-stone.digital/\n",
            "8                                   \n",
            "9       https://cryptomainettapps.in\n",
            "Name: Phishing_URL, dtype: object\n"
          ]
        }
      ],
      "source": [
        "df['Phishing_URL'] = tweets.apply(extract_phishing_url)\n",
        "\n",
        "# Debug: Print the first few cleaned URLs to check results\n",
        "print(\"Sample Phishing URLs:\")\n",
        "print(df['Phishing_URL'].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN5tN8bxO6aN"
      },
      "source": [
        "6. Filter out rows where no phishing URL was found and keep the cleaned URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Al8rZb0OO8hs"
      },
      "outputs": [],
      "source": [
        "df = df[df['Phishing_URL'] != '']\n",
        "\n",
        "# Keep only the column with the cleaned URLs\n",
        "cleaned_df = df[['Phishing_URL']] # Adjust the number of URLs based on the research purpose"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkHJ5jOyPJ-9"
      },
      "source": [
        "**Save the cleaned phishing URLs to a new CSV file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJ2eyaf9PLK8",
        "outputId": "19137105-f79e-449f-cebd-a5868486dd8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned CSV file has been created.\n"
          ]
        }
      ],
      "source": [
        "cleaned_df.to_csv('/content/drive/MyDrive/Thesis - Undergraduate Ch./Dataset/Phishing/Cleaned/cleaned_phishing_url_batch1.csv', index=False)\n",
        "\n",
        "print(\"Cleaned CSV file has been created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwT2aq3bXCWZ"
      },
      "source": [
        "[Additional] Concantenate the cleaned data from different dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaRHT8wwqFF7",
        "outputId": "69bd4496-fb80-4c2e-91a6-a72c223b78c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The 1500 most suspicious phishing URLs have been saved! Total samples: 2785\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load Datasets\n",
        "crawled_df = pd.read_csv('/content/drive/MyDrive/Thesis - Undergraduate Ch./Dataset/Phishing/Cleaned/cleaned_phishing_url.csv')\n",
        "urlscan_df = pd.read_csv('/content/drive/MyDrive/Thesis - Undergraduate Ch./Dataset/Phishing/Cleaned/cleaned_URLScanio.csv')\n",
        "\n",
        "# Lists for Filtering\n",
        "phishy_keywords = [\n",
        "    'login', 'signin', 'verify', 'secure', 'account', 'update', 'submit',\n",
        "    'invoice', 'bank', 'password', 'confirmation', 'security-check', 'validate'\n",
        "]\n",
        "\n",
        "impersonated_brands = ['paypal', 'google', 'facebook', 'amazon', 'apple', 'microsoft']\n",
        "\n",
        "suspicious_tlds = ['.xyz', '.club', '.top', '.gq', '.tk', '.ml', '.cf', '.ga']\n",
        "\n",
        "# Filtering Functions\n",
        "def contains_phishy_keywords(url):\n",
        "    return any(kw in url.lower() for kw in phishy_keywords)\n",
        "\n",
        "def contains_suspicious_characters(url):\n",
        "    return len(re.findall(r'[@$%&!^*()<>?;]', url)) > 1\n",
        "\n",
        "def is_short_and_suspicious(url):\n",
        "    return len(url) < 40 and contains_suspicious_characters(url)\n",
        "\n",
        "def is_long_and_suspicious(url):\n",
        "    return len(url) >= 40 and contains_suspicious_characters(url)\n",
        "\n",
        "def is_short_with_sneaky_pattern(url):\n",
        "    return len(url) < 40 and contains_suspicious_characters(url) and contains_phishy_keywords(url)\n",
        "\n",
        "def starts_suspicious_path(url):\n",
        "    path_match = re.findall(r'//[^/]+(/[^?]*)', url)\n",
        "    if path_match:\n",
        "        suspicious_starts = ['/login', '/account', '/secure', '/update']\n",
        "        return any(p in path_match[0].lower() for p in suspicious_starts)\n",
        "    return False\n",
        "\n",
        "def has_weird_subdomains(url):\n",
        "    match = re.findall(r'//([^/]+)', url)\n",
        "    if match:\n",
        "        domain = match[0]\n",
        "        subdomains = domain.split('.')\n",
        "        return len(subdomains) >= 4\n",
        "    return False\n",
        "\n",
        "def contains_obscure_unicode(url):\n",
        "    return bool(re.search(r'%[0-9a-fA-F]{2}', url)) or 'xn--' in url\n",
        "\n",
        "def contains_brand_typo(url):\n",
        "    url_lower = url.lower()\n",
        "    for brand in impersonated_brands:\n",
        "        if brand in url_lower:\n",
        "            if re.search(r'[01]', url_lower) or brand not in re.split(r'\\W+', url_lower):\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "def uses_suspicious_tld(url):\n",
        "    return any(url.lower().endswith(tld) for tld in suspicious_tlds)\n",
        "\n",
        "def has_many_paths_or_params(url):\n",
        "    parsed = urlparse(url)\n",
        "    num_paths = parsed.path.count('/')\n",
        "    num_params = len(parse_qs(parsed.query))\n",
        "    return num_paths > 5 or num_params > 5\n",
        "\n",
        "# Master Filter Function \n",
        "def url_is_suspicious(url):\n",
        "    return any([\n",
        "        is_short_with_sneaky_pattern(url),\n",
        "        starts_suspicious_path(url),\n",
        "        has_weird_subdomains(url),\n",
        "        contains_obscure_unicode(url),\n",
        "        contains_brand_typo(url),\n",
        "        uses_suspicious_tld(url),\n",
        "        has_many_paths_or_params(url),\n",
        "    ])\n",
        "\n",
        "# Apply Filtering\n",
        "# Custom suspicious filters\n",
        "urlscan_df_custom_filtered = urlscan_df[\n",
        "    urlscan_df['Phishing_URL'].apply(url_is_suspicious)\n",
        "]\n",
        "\n",
        "# Filter short and long URLs based on the criteria \n",
        "urlscan_df_filtered_short = urlscan_df[urlscan_df['Phishing_URL'].apply(is_short_and_suspicious)]\n",
        "urlscan_df_filtered_long = urlscan_df[urlscan_df['Phishing_URL'].apply(is_long_and_suspicious)]\n",
        "\n",
        "short_limit = len(urlscan_df_filtered_short) // 2\n",
        "long_limit = len(urlscan_df_filtered_long) // 2\n",
        "\n",
        "urlscan_df_filtered = pd.concat([\n",
        "    urlscan_df_filtered_short.head(short_limit),\n",
        "    urlscan_df_filtered_long.head(long_limit),\n",
        "    urlscan_df_custom_filtered\n",
        "]).drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "# Combine with Crawled Dataset\n",
        "combined_df = pd.concat([crawled_df, urlscan_df_filtered], ignore_index=True)\n",
        "combined_df = combined_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Sort and Select Most Suspicious URLs \n",
        "# Sorting without adding a new column\n",
        "sorted_combined_df = combined_df[combined_df['Phishing_URL'].apply(url_is_suspicious)].sort_values(by='Phishing_URL', ascending=False)\n",
        "\n",
        "# Select the top suspicious URLs\n",
        "top_suspicious_df = sorted_combined_df.head(5000)\n",
        "\n",
        "# Save the filtered dataset\n",
        "top_suspicious_df.to_csv('/content/drive/MyDrive/Thesis - Undergraduate Ch./Dataset/Phishing/Cleaned/combined_phishing_url_5000.csv', index=False)\n",
        "\n",
        "print(f\"The suspicious phishing URLs have been saved! Total samples: {len(top_suspicious_df)}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
